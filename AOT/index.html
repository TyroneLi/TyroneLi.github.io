<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <meta name="description"
        content="Token Reduction via Local and Global Contexts Optimization for Efficient Video Large Language Models">
  <meta name="keywords" content="Token Reduction, Token Compression, Token Pruning, Video LLM, Video Large Language Model, Optimal Transport">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Token Reduction via Local and Global Contexts Optimization for Efficient Video Large Language Models</title>

  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icons/emova.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/progressive-image.js/dist/progressive-image.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/d3915a16e2.js" crossorigin="anonymous"></script>

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>

	<!-- <style>
		.chat-history {
			flex-grow: 1;
			overflow-y: auto;
			/* overflow-x: hidden; */
			padding: 5px;
			border-bottom: 1px solid #ccc;
			margin-bottom: 10px;
			text-align: left;
		}

		.chat-history img {
			display: none;
			max-width: 100%;
			max-height: 100%;
		}

		.chat-history img.active {
			display: block;
		}
	</style> -->
  	
  <style>
		.chat-history {
			flex-grow: 1;
			overflow-y: auto;
			/* overflow-x: hidden; */
			padding: 5px;
			border-bottom: 1px solid #ccc;
			margin-bottom: 10px;
			text-align: left;
		}

		.chat-history figure {
			display: none;
			max-width: 100%;
			max-height: 100%;
		}

		.chat-history figure.active {
			display: block;
		}

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
    
	</style>
  
<body>


  

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title"><b>AOT:</b>  </h1> -->
          <h2 class="title is-1 publication-title"><b>AOT:</b>  </h2>
          <h3 class="title is-3 publication-title"> Token Reduction via Local and Global Contexts Optimization for Efficient Video Large Language Models </h3>
          <!-- <h2 class="title is-3 publication-title"> with Vivid Emotions   </h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tyroneli.github.io/">Jinlong Li</a><sup>1,†</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Liyuan_Jiang1">Liyuan Jiang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://zchoi.github.io/">Haonan Zhang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en">Nicu Sebe</a><sup>1</sup>.</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Trento,</span>
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>University of Electronic Science and Technology of China.</span>
            <br>
          </div>
          <!-- <div class="is-size-6 publication-authors">
            <span class="author-block">(<sup>*</sup>Equal contribution.
            <sup><span>&#8224;</span></sup>Corresponding authors.
            )</span>
          </div> -->

          <h2 align="center"><strong><span style="color: red; font-size: 24px;">Arxiv 2026</span></strong></h2>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.16707"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/TyroneLi/AOT"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -100px;">
  <!-- <div class="container pt-5 mt-0 shadow p-5 mb-0 bg-white rounded" style="width: 80%;"> -->
  <div class="container" style="width: 90%;">
    <!-- conflicts. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <!-- <h2 class="title is-3">General Capabilities & Generalization Ability</h2> -->
        <div class="teaser">
          <img src="./static/images/AOT_motivation.png"  width="800" height="800">
        </div>
        <p><br/></p>  
        <div class="content has-text-justified">
          <p>
            The top is the essential differences compared with common token reduction methods, 
            instead of simply removing unimportant or merging very similar tokens, ours utilizes 
            a global optimization strategy to further exploit and aggregate necessary semantic 
            and context from these onto the remaining tokens. Bottom
            is our proposed pipeline to adopt Optimal Transport to aggregate
            information within intra- and inter-frame levels for video tokens.
          </p>
        </div>
      </div>
    </div>
    <!--/ safety persists. -->
  </div>
</section>

<section class="section" style="background-color: #f1f1f1;">
    <!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded" style="width: 50%;"> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered" style="width: 65%; margin: 0 auto;">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video Large Language Models (VLLMs) demonstrate
            strong video understanding but suffer from inefficiency due
            to redundant visual tokens. Existing pruning primary targets 
            intra-frame spatial redundancy or prunes inside the
            LLM with shallow-layer overhead, yielding suboptimal spatiotemporal 
            reduction and underutilizing long-context compressibility. 
            All of them often discard subtle yet informative
            context from merged or pruned tokens. In this paper, we
            propose a new perspective that elaborates token Anchors
            within intra-frame and inter-frame to comprehensively aggregate 
            the informative contexts via local-global Optimal
            Transport (AOT). Specifically, we first establish local- and
            global-aware token anchors within each frame under the attention 
            guidance, which then optimal transport aggregates
            the informative contexts from pruned tokens, constructing
            intra-frame token anchors. Then, building on the temporal frame 
            clips, the first frame within each clip will be considered as the 
            keyframe anchors to ensemble similar information from 
            consecutive frames through optimal transport,
            while keeping distinct tokens to represent temporal dynamics, 
            leading to efficient token reduction in a training-free
            manner. Extensive evaluations show that our proposed AOT
            obtains competitive performances across various short- and
            long-video benchmarks on leading video LLMs, obtaining
            substantial computational efficiency while preserving temporal and visual fidelity.
          </p> 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <!-- </div> -->
</section>


<section class="section">
  <!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded" style="width: 50%;"> -->
    <!-- conflicts. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Overview of CUA-O3D</h2>
        <div class="teaser">
          <img src="./static/images/AOT_pipeline.png"  width="1100" height="1100">
        </div>
        <p><br/></p>  
        <div class="content has-text-justified" style="width: 65%; margin: 0 auto;">
          <p>
            Overall pipeline of our AOT. Our method compresses tokens of video LLMs across spatiotemporal through optimal transport,
          first establishing token anchors within each frame to cover semantically important and spatially diverse token candidates, then utilizing
          optimal transport to aggregate the necessary informative cues within Intra-Frame at phase I, and finally shifting the optimization strategy
          into temporal within Inter-Frame at phase II. The proposed AOT preserves both temporal and visual integrity by utilizing efficient 
          SinkhornKnopp Iteration to solve the optimal transport plan assignment.
          </p>
        </div>
      </div>
    </div>
    <!--/ safety persists. -->
  <!-- </div> -->
</section>
<hr>
<section class="section" >
    <!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded" style="width: 50%;"> -->
        <div class="columns is-centered has-text-centered" style="width: 80%; margin: 0 auto;">
          <div class="column is-five-fifths">
            <h2 class="title is-3">Video Performance & Reduction Ability</h2>
        <div class="teaser">
          <img src="./static/images/AOT_LLaVAOneVision7B.png" width="1600" height="1000">
        </div>
        <div class="content has-text-justified">
        <p>
          Table 1. Comparison of state-of-the-art methods on LLaVA-OneVision across video benchmarks. The best performance among those
          with similar retention ratios Ratio is highlighted in bold, while the second best will be denoted as underlined.
				<!-- <sup>∗</sup> indicates max((Accuracy+MAP+BLEU-MAE)/4, 0). -->
        </p>
        </div>
        <hr>
        <div class="teaser">
          <img src="./static/images/AOT_LLaVAVideo7B.png" width="1600" height="1200">
        </div>
        <div class="content has-text-justified">
          <p>
            Table 2. Comparison of state-of-the-art methods on LLaVA-Video across video benchmarks. The best performance among those is
            highlighted in bold, while the second best will be denoted as underlined, demonstrating consistent effectiveness.
          </p>
        </div>
        <hr>
        <div class="teaser">
          <img src="./static/images/AOT_LLaVAOneVision7B_Dync.png" width="1600" height="1000">
        </div>
        <div class="content has-text-justified">
          <p>
            Table 3. Comparison of state-of-the-art methods on LLaVA-OneVision across video benchmarks. The best performance among those
            with similar retention ratios Ratio is highlighted in bold, while the second best will be denoted as underlined. AOT w Dyn denotes we
            apply dynamic temporal segmentation to obtain adaptive frames within each clip, following FastVID.
          </p>
        </div>
        <hr>
        <div class="teaser">
          <img src="./static/images/AOT_LLaVAVideo7B_Dync.png" width="1600" height="1000">
        </div>
        <div class="content has-text-justified">
          <p>
            Table 4. Comparison of state-of-the-art methods on LLaVA-Video across video benchmarks. The best performance among those is
            highlighted in bold, while the second best will be denoted as underlined, demonstrating consistent effectiveness. AOT w Dyn denotes we
            apply dynamic temporal segmentation to obtain adaptive frames within each clip, following FastVID.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</section>
<hr>
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Visualizations</h2>
    </div>
  </div>
  <div class="container mt-0">
    <div class="form-row" style="justify-content: center;">
      <div class="form-group col-md-1">
        <!-- <div class="col-md-0" style="width: 100%"><label>&nbsp;</label></div> -->
        <div class="btn-group" role="group" aria-label="Left and Right Controller"
          style="width: 100%;align-items: flex-end;justify-content: center;flex-direction: row;display: flex;">
          <button type="button" class="form-control btn btn-primary" id="prev-question" style="background-color: black; color: white; border-color: black;"><i
              class="material-icons">keyboard_arrow_left</i></button>
          <button type="button" class="form-control btn btn-primary" id="next-question" style="background-color: black; color: white; border-color: black;"><i
              class="material-icons">keyboard_arrow_right</i></button>
  
        </div>
      </div>
    </div>
  
    <div style="display: flex; justify-content: center; align-items: center;">
      <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
        <div class="card-body" id="selected-question" style="display: flex; height: 100vh;">
          <div class="chat-history">
            <article class="media">
              <figure class="active">
                <img src="./static/visualization/AOT_demo01.png">
                <figcaption style="font-size: 23px;"><sup> Qualitative visualizations of our Local-Global token anchors evolution across consecutive frames on MVBench sample while
                  optimal transport is adopted to aggregate necessary information from unselected tokens to help LLM precess better. The top is the original
                  sampled frames while the bottom is the corresponding tokens visualization.</sup></figcaption>
              </figure>
              <figure class="active">
                <img src="./static/visualization/AOT_demo02.png">
                <figcaption style="font-size: 23px;"><sup> Qualitative visualizations of our Local-Global token anchors evolution across consecutive frames on MVBench sample while
                  optimal transport is adopted to aggregate necessary information from unselected tokens to help LLM precess better. The top is the original
                  sampled frames while the bottom is the corresponding tokens visualization.</sup></figcaption>
              </figure>
              <figure class="active">
                <img src="./static/visualization/AOT_demo03.png">
                <figcaption style="font-size: 23px;"><sup> Qualitative visualizations of our Local-Global token anchors evolution across consecutive frames on MVBench sample while
                  optimal transport is adopted to aggregate necessary information from unselected tokens to help LLM precess better. The top is the original
                  sampled frames while the bottom is the corresponding tokens visualization.</sup></figcaption>
              </figure>
              <figure class="active">
                <img src="./static/visualization/AOT_demo04.png">
                <figcaption style="font-size: 23px;"><sup> Qualitative visualizations of our Local-Global token anchors evolution across consecutive frames on MVBench sample while
                  optimal transport is adopted to aggregate necessary information from unselected tokens to help LLM precess better. The top is the original
                  sampled frames while the bottom is the corresponding tokens visualization.</sup></figcaption>
              </figure>
						</article>
            <!-- Add your chat messages here -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX" >
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>
        @inproceedings{li2025cross,
          title={Cross-modal and uncertainty-aware agglomeration for open-vocabulary 3d scene understanding},
          author={Li, Jinlong and Saltori, Cristiano and Poiesi, Fabio and Sebe, Nicu},
          booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
          pages={19390--19400},
          year={2025}
        }
      </code>
    </pre>
  </div>
</section>

<script>
  document.addEventListener('DOMContentLoaded', function () {
    const images = document.querySelectorAll('.chat-history figure');
    let currentIndex = 0;

    document.getElementById('prev-question').addEventListener('click', function () {
      images[currentIndex].classList.remove('active');
      currentIndex = (currentIndex === 0) ? images.length - 1 : currentIndex - 1;
      images[currentIndex].classList.add('active');
    });

    document.getElementById('next-question').addEventListener('click', function () {
      images[currentIndex].classList.remove('active');
      currentIndex = (currentIndex === images.length - 1) ? 0 : currentIndex + 1;
      images[currentIndex].classList.add('active');
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<div align="center">
  <a href="https://info.flagcounter.com/4hf5">
    <img src="https://s01.flagcounter.com/mini/4hf5/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>
</div>

<footer class="footer" style="background-color: #f1f1f1;">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2503.16707">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/TyroneLi/AOT" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <center>The website template was adapted from <a href="https://nerfies.github.io/">Nerfies</a>.<br/>
          @ CUA_O3D Team
          </center>
        <p></p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
